{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nahbos/AUT-Neural-Networks/blob/main/HW08/HW08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNxi5T17_epW"
      },
      "source": [
        "- Sobhan Moradian Daghigh\n",
        "- 7/12/2022\n",
        "- ANN - HW08"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "nb4lrkXRHNyO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5A65U0h_LVy",
        "outputId": "b06f0373-46de-41c2-b64e-d76b99c2aba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-text==2.8.* in /usr/local/lib/python3.7/dist-packages (2.8.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.26.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.21.6)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.47.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (14.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.1.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U 'tensorflow-text==2.8.*'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "Tmv7BGIn_jUB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "wtbvvEmv_jXJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRvxt_6FJadW"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "SYYU2Xim_jbR"
      },
      "outputs": [],
      "source": [
        "dataset = tfds.load('ted_hrlr_translate/pt_to_en', as_supervised=True)\n",
        "x_train, x_test, x_val = dataset['train'], dataset['test'], dataset['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "bPrIqinV_jep"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decrease size of the training data"
      ],
      "metadata": {
        "id": "5R8JHPMsK5sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 20000"
      ],
      "metadata": {
        "id": "f2pnS9paNpBC"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_len = len(list(x_train))\n",
        "x_train_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcRNbHSAJxE5",
        "outputId": "8e8b4ed3-d58f-42b6-d221-0d6ec3da96b8"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51785"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentage = 0.33\n",
        "x_train = x_train.shuffle(BUFFER_SIZE).take(int(x_train_len * percentage))\n",
        "len(list(x_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwUIgH1LJ18J",
        "outputId": "33e4818d-1f81-4f07-9544-cc115ebed237"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17089"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TaV5r9JmKIoi"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSUwHICIJmnP"
      },
      "source": [
        "### Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D9rjCLr_jhq",
        "outputId": "5839af7e-635c-4b99-9157-ac0463ba16c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Portuguese: [uma vez , no entanto , ele aceitou fazer documentos falsos para alguém que vocês irão , talvez , reconhecer .]\n",
            "English:    [once , though , he agreed to make false papers for someone you might recognize .]\n",
            "---------------------------------\n",
            "Portuguese: [não sabemos como agir eficazmente .]\n",
            "English:    [we do n't know how to take effective action .]\n",
            "---------------------------------\n",
            "Portuguese: [`` neste momento , a maioria das pessoas estão a pensar em `` '' sexting '' '' sem pensarem realmente no consentimento . '']\n",
            "English:    [right now most people are thinking about sexting without really thinking about consent at all .]\n",
            "---------------------------------\n",
            "Portuguese: [debaixo do nosso apartamento , há um carpinteiro , lojas de guloseimas , um talho , uma gráfica , oficinas e muitas mais lojas .]\n",
            "English:    [under our apartment , there is a carpenter , sweetshops , a butcher , a printing house , workshops , among many more .]\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "for pt_sentences, en_sentences in x_train.batch(4).take(1):\n",
        "  for pt, en in zip(pt_sentences.numpy(), en_sentences.numpy()):\n",
        "    print('Portuguese: [{}]'.format(pt.decode('utf-8')))\n",
        "    print('English:    [{}]'.format(en.decode('utf-8')))\n",
        "    print('---------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "GgrWv9G9_jnh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBUT79SvL5p3"
      },
      "source": [
        "# Text tokenization\n",
        "Downloading a model which was implemented for tokenizing the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "n61hrZzb_jqE",
        "outputId": "cd623c0f-fa38-49a0-a040-dea9b54365da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./ted_hrlr_translate_pt_en_converter.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 179
        }
      ],
      "source": [
        "name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.keras.utils.get_file(\n",
        "    f'{name}.zip',\n",
        "    f'https://storage.googleapis.com/download.tensorflow.org/models/{name}.zip',\n",
        "    cache_dir='.', cache_subdir='', extract=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "K0LTsqZ8_jtM"
      },
      "outputs": [],
      "source": [
        "tokenizers = tf.saved_model.load(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEAeGcrHWII8"
      },
      "source": [
        "### Lets see the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaiANy99_jwJ",
        "outputId": "54992491-5f46-49f7-f351-d27433b4bf4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English:   [and building is the next of the behaviors that i thought i’d talk about .]\n",
            "English:   [i spoke to drugsheaven .]\n",
            "Tokenized: [[2, 72, 390, 80, 71, 261, 74, 71, 1921, 75, 45, 275, 45, 68, 40, 229, 95, 15, 3]]\n",
            "Tokenized: [[2, 45, 3533, 73, 1233, 5501, 3410, 269, 15, 3]]\n"
          ]
        }
      ],
      "source": [
        "for pt_sentences, en_sentences in x_train.batch(2).take(1):\n",
        "  for en in en_sentences.numpy():\n",
        "    print('English:   [{}]'.format(en.decode('utf-8')))\n",
        "\n",
        "  tokenized = tokenizers.en.tokenize(en_sentences)\n",
        "  for sentence in tokenized.to_list():\n",
        "    print('Tokenized: [{}]'.format(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "7HT2hEZl_jzC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnWq3n13ZP5S"
      },
      "source": [
        "# Setup input pipeline\n",
        "* Now lets make a filter on the sentences which have shorter than a threshold\n",
        "* Then make batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "Jx41goy66OeS"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS  = 128\n",
        "BATCH_SIZE  = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "1BRCAa1u_j1q"
      },
      "outputs": [],
      "source": [
        "def filter_max_tokens(pt, en):\n",
        "  num_tokens = tf.maximum(tf.shape(pt)[1],tf.shape(en)[1])\n",
        "  return num_tokens < MAX_TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "2LlJ1bGc_j4h"
      },
      "outputs": [],
      "source": [
        "def tokenize_pairs(pt, en):\n",
        "    pt = tokenizers.pt.tokenize(pt)\n",
        "    pt = pt.to_tensor()\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    en = en.to_tensor()\n",
        "    \n",
        "    return pt, en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "XOFMmGFS_j7B"
      },
      "outputs": [],
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(tf.data.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "aw4IGtGA_j96"
      },
      "outputs": [],
      "source": [
        "train_batches = make_batches(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B9N3yPj5JJJ4"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cN0seSv-uiq"
      },
      "source": [
        "# Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "rq6kFEHp_kDi"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "v7t1G6Ul_kGK"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
        "\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1iq_t8N_kJB",
        "outputId": "29f0e694-56cf-4b77-cd77-b3d713fec0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 2048, 512)\n"
          ]
        }
      ],
      "source": [
        "n, d = 2048, 512\n",
        "pos_encoding = positional_encoding(n, d)\n",
        "print(pos_encoding.shape)\n",
        "pos_encoding = pos_encoding[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "Z6DHFdrf_kLr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzUwP2Ct_6lZ"
      },
      "source": [
        "# Mask pad tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "Eapm6q-w_kdr"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "tCiQ7WKW_kf_"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "xNUevuzX_kim"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "Ix0OCZ5M_ksn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "WNEhqWnf_kvJ"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)\n",
        "\n",
        "  return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "Ix8yOPrQ_kxh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "AA-7ZmfK_k0H"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)\n",
        "    k = self.wk(k)\n",
        "    v = self.wv(v)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)\n",
        "    k = self.split_heads(k, batch_size)\n",
        "    v = self.split_heads(v, batch_size)\n",
        "\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "    concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))\n",
        "\n",
        "    output = self.dense(concat_attention)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "gF8SgyQY_k2y"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "VNv8umtr_k5L"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "TmH2nKXR_k7z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "vAVSeieC_k-i"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "RfViBVAX_lBK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "G8vntxk2_lDt"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    \n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "    ffn_output = self.ffn(out2)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "V9hpiWEb_lGT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "CrqmdrUc_lJL"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    \n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "W0_9bJ6S_lL5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "buQNHJUd_kOn"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(MAX_TOKENS, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    return x, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "OHnsKL9i_kQ-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "UnDu2tbt_kUD"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           input_vocab_size=input_vocab_size, rate=rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           target_vocab_size=target_vocab_size, rate=rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "\n",
        "    inp, tar = inputs\n",
        "\n",
        "    padding_mask, look_ahead_mask = self.create_masks(inp, tar)\n",
        "\n",
        "    enc_output = self.encoder(inp, training, padding_mask)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  def create_masks(self, inp, tar):\n",
        "    padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return padding_mask, look_ahead_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "QyfCkG9hCp5X"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "oN8pYRv9CqHr"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "Vffhg74dCqKE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "nM6_27M3CqMi"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "vFf854kSCqPI"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "IZt-c2WICqRs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "_rp4Qb01CqUJ"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "5CoPib6bCqW6"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "sNaU_wnJCqZf"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "mtAXtChGCqcO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "QfnT5PzACqee"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "Pn8Ckgh9Cqgq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "_1JHdUOgCqjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c481caff-41c0-47dd-f354-e1fc4c839d1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest checkpoint restored!!\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = './checkpoints/train'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "GEpExJZvC92f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "M9-yXqJpC-Ll"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "KqXNpl_vC-Nw"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "e4WosirVC-QO"
      },
      "outputs": [],
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer([inp, tar_inp],\n",
        "                                 training = True)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "ErpCoGfLC-TF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzXey1k4C-Vq",
        "outputId": "04358def-24ee-4711-c4df-c00d836eb40b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2087 Accuracy 0.5979\n",
            "Epoch 1 Batch 50 Loss 2.5863 Accuracy 0.5494\n",
            "Epoch 1 Batch 100 Loss 2.5749 Accuracy 0.5456\n",
            "Epoch 1 Batch 150 Loss 2.5624 Accuracy 0.5446\n",
            "Epoch 1 Batch 200 Loss 2.5450 Accuracy 0.5435\n",
            "Epoch 1 Loss 2.5525 Accuracy 0.5411\n",
            "Time taken for 1 epoch: 53.03 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.9103 Accuracy 0.6203\n",
            "Epoch 2 Batch 50 Loss 2.1266 Accuracy 0.5839\n",
            "Epoch 2 Batch 100 Loss 2.1646 Accuracy 0.5795\n",
            "Epoch 2 Batch 150 Loss 2.1727 Accuracy 0.5769\n",
            "Epoch 2 Batch 200 Loss 2.1801 Accuracy 0.5751\n",
            "Epoch 2 Loss 2.1810 Accuracy 0.5745\n",
            "Time taken for 1 epoch: 26.19 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.8562 Accuracy 0.6306\n",
            "Epoch 3 Batch 50 Loss 1.9293 Accuracy 0.6046\n",
            "Epoch 3 Batch 100 Loss 1.9221 Accuracy 0.6064\n",
            "Epoch 3 Batch 150 Loss 1.9520 Accuracy 0.6019\n",
            "Epoch 3 Batch 200 Loss 1.9642 Accuracy 0.5994\n",
            "Epoch 3 Loss 1.9661 Accuracy 0.5992\n",
            "Time taken for 1 epoch: 26.41 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6325 Accuracy 0.6625\n",
            "Epoch 4 Batch 50 Loss 1.7618 Accuracy 0.6279\n",
            "Epoch 4 Batch 100 Loss 1.7718 Accuracy 0.6249\n",
            "Epoch 4 Batch 150 Loss 1.7855 Accuracy 0.6222\n",
            "Epoch 4 Batch 200 Loss 1.8000 Accuracy 0.6195\n",
            "Epoch 4 Loss 1.8131 Accuracy 0.6175\n",
            "Time taken for 1 epoch: 26.25 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5003 Accuracy 0.6567\n",
            "Epoch 5 Batch 50 Loss 1.6309 Accuracy 0.6432\n",
            "Epoch 5 Batch 100 Loss 1.6335 Accuracy 0.6429\n",
            "Epoch 5 Batch 150 Loss 1.6466 Accuracy 0.6410\n",
            "Epoch 5 Batch 200 Loss 1.6661 Accuracy 0.6376\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-13\n",
            "Epoch 5 Loss 1.6874 Accuracy 0.6348\n",
            "Time taken for 1 epoch: 26.48 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3975 Accuracy 0.6924\n",
            "Epoch 6 Batch 50 Loss 1.5018 Accuracy 0.6665\n",
            "Epoch 6 Batch 100 Loss 1.5118 Accuracy 0.6636\n",
            "Epoch 6 Batch 150 Loss 1.5287 Accuracy 0.6596\n",
            "Epoch 6 Batch 200 Loss 1.5475 Accuracy 0.6553\n",
            "Epoch 6 Loss 1.5631 Accuracy 0.6521\n",
            "Time taken for 1 epoch: 25.80 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.5349 Accuracy 0.6582\n",
            "Epoch 7 Batch 50 Loss 1.3813 Accuracy 0.6832\n",
            "Epoch 7 Batch 100 Loss 1.4039 Accuracy 0.6791\n",
            "Epoch 7 Batch 150 Loss 1.4288 Accuracy 0.6746\n",
            "Epoch 7 Batch 200 Loss 1.4566 Accuracy 0.6690\n",
            "Epoch 7 Loss 1.4710 Accuracy 0.6656\n",
            "Time taken for 1 epoch: 26.26 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2084 Accuracy 0.7159\n",
            "Epoch 8 Batch 50 Loss 1.3124 Accuracy 0.6935\n",
            "Epoch 8 Batch 100 Loss 1.3324 Accuracy 0.6905\n",
            "Epoch 8 Batch 150 Loss 1.3519 Accuracy 0.6861\n",
            "Epoch 8 Batch 200 Loss 1.3702 Accuracy 0.6824\n",
            "Epoch 8 Loss 1.3805 Accuracy 0.6806\n",
            "Time taken for 1 epoch: 25.86 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.0050 Accuracy 0.7567\n",
            "Epoch 9 Batch 50 Loss 1.2262 Accuracy 0.7085\n",
            "Epoch 9 Batch 100 Loss 1.2583 Accuracy 0.7020\n",
            "Epoch 9 Batch 150 Loss 1.2768 Accuracy 0.6987\n",
            "Epoch 9 Batch 200 Loss 1.2992 Accuracy 0.6942\n",
            "Epoch 9 Loss 1.3147 Accuracy 0.6912\n",
            "Time taken for 1 epoch: 26.62 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.1278 Accuracy 0.7441\n",
            "Epoch 10 Batch 50 Loss 1.1658 Accuracy 0.7164\n",
            "Epoch 10 Batch 100 Loss 1.1819 Accuracy 0.7144\n",
            "Epoch 10 Batch 150 Loss 1.1993 Accuracy 0.7112\n",
            "Epoch 10 Batch 200 Loss 1.2249 Accuracy 0.7064\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-14\n",
            "Epoch 10 Loss 1.2400 Accuracy 0.7041\n",
            "Time taken for 1 epoch: 27.63 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.1286 Accuracy 0.7394\n",
            "Epoch 11 Batch 50 Loss 1.1120 Accuracy 0.7302\n",
            "Epoch 11 Batch 100 Loss 1.1202 Accuracy 0.7269\n",
            "Epoch 11 Batch 150 Loss 1.1434 Accuracy 0.7212\n",
            "Epoch 11 Batch 200 Loss 1.1640 Accuracy 0.7170\n",
            "Epoch 11 Loss 1.1872 Accuracy 0.7129\n",
            "Time taken for 1 epoch: 25.99 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.1004 Accuracy 0.7398\n",
            "Epoch 12 Batch 50 Loss 1.0602 Accuracy 0.7368\n",
            "Epoch 12 Batch 100 Loss 1.0718 Accuracy 0.7341\n",
            "Epoch 12 Batch 150 Loss 1.0855 Accuracy 0.7319\n",
            "Epoch 12 Batch 200 Loss 1.1098 Accuracy 0.7271\n",
            "Epoch 12 Loss 1.1216 Accuracy 0.7243\n",
            "Time taken for 1 epoch: 26.23 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.9131 Accuracy 0.7644\n",
            "Epoch 13 Batch 50 Loss 1.0147 Accuracy 0.7459\n",
            "Epoch 13 Batch 100 Loss 1.0248 Accuracy 0.7442\n",
            "Epoch 13 Batch 150 Loss 1.0382 Accuracy 0.7410\n",
            "Epoch 13 Batch 200 Loss 1.0564 Accuracy 0.7365\n",
            "Epoch 13 Loss 1.0750 Accuracy 0.7323\n",
            "Time taken for 1 epoch: 26.06 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.8604 Accuracy 0.7804\n",
            "Epoch 14 Batch 50 Loss 0.9807 Accuracy 0.7507\n",
            "Epoch 14 Batch 100 Loss 0.9877 Accuracy 0.7503\n",
            "Epoch 14 Batch 150 Loss 1.0027 Accuracy 0.7469\n",
            "Epoch 14 Batch 200 Loss 1.0167 Accuracy 0.7440\n",
            "Epoch 14 Loss 1.0300 Accuracy 0.7410\n",
            "Time taken for 1 epoch: 26.17 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.8255 Accuracy 0.7893\n",
            "Epoch 15 Batch 50 Loss 0.9075 Accuracy 0.7685\n",
            "Epoch 15 Batch 100 Loss 0.9237 Accuracy 0.7641\n",
            "Epoch 15 Batch 150 Loss 0.9450 Accuracy 0.7591\n",
            "Epoch 15 Batch 200 Loss 0.9718 Accuracy 0.7532\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-15\n",
            "Epoch 15 Loss 0.9824 Accuracy 0.7505\n",
            "Time taken for 1 epoch: 26.53 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.7622 Accuracy 0.8000\n",
            "Epoch 16 Batch 50 Loss 0.8681 Accuracy 0.7760\n",
            "Epoch 16 Batch 100 Loss 0.8868 Accuracy 0.7703\n",
            "Epoch 16 Batch 150 Loss 0.9051 Accuracy 0.7661\n",
            "Epoch 16 Batch 200 Loss 0.9255 Accuracy 0.7613\n",
            "Epoch 16 Loss 0.9399 Accuracy 0.7581\n",
            "Time taken for 1 epoch: 26.07 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.6991 Accuracy 0.8151\n",
            "Epoch 17 Batch 50 Loss 0.8550 Accuracy 0.7768\n",
            "Epoch 17 Batch 100 Loss 0.8562 Accuracy 0.7777\n",
            "Epoch 17 Batch 150 Loss 0.8738 Accuracy 0.7731\n",
            "Epoch 17 Batch 200 Loss 0.8972 Accuracy 0.7675\n",
            "Epoch 17 Loss 0.9109 Accuracy 0.7642\n",
            "Time taken for 1 epoch: 26.05 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.7160 Accuracy 0.8153\n",
            "Epoch 18 Batch 50 Loss 0.8072 Accuracy 0.7872\n",
            "Epoch 18 Batch 100 Loss 0.8301 Accuracy 0.7816\n",
            "Epoch 18 Batch 150 Loss 0.8489 Accuracy 0.7776\n",
            "Epoch 18 Batch 200 Loss 0.8612 Accuracy 0.7746\n",
            "Epoch 18 Loss 0.8721 Accuracy 0.7718\n",
            "Time taken for 1 epoch: 26.33 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.7497 Accuracy 0.8148\n",
            "Epoch 19 Batch 50 Loss 0.7752 Accuracy 0.7944\n",
            "Epoch 19 Batch 100 Loss 0.7906 Accuracy 0.7912\n",
            "Epoch 19 Batch 150 Loss 0.8072 Accuracy 0.7870\n",
            "Epoch 19 Batch 200 Loss 0.8255 Accuracy 0.7825\n",
            "Epoch 19 Loss 0.8382 Accuracy 0.7797\n",
            "Time taken for 1 epoch: 25.87 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.7409 Accuracy 0.7941\n",
            "Epoch 20 Batch 50 Loss 0.7485 Accuracy 0.7996\n",
            "Epoch 20 Batch 100 Loss 0.7592 Accuracy 0.7974\n",
            "Epoch 20 Batch 150 Loss 0.7806 Accuracy 0.7923\n",
            "Epoch 20 Batch 200 Loss 0.8015 Accuracy 0.7871\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-16\n",
            "Epoch 20 Loss 0.8164 Accuracy 0.7836\n",
            "Time taken for 1 epoch: 26.29 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.7229 Accuracy 0.8110\n",
            "Epoch 21 Batch 50 Loss 0.7355 Accuracy 0.8024\n",
            "Epoch 21 Batch 100 Loss 0.7440 Accuracy 0.7991\n",
            "Epoch 21 Batch 150 Loss 0.7565 Accuracy 0.7963\n",
            "Epoch 21 Batch 200 Loss 0.7723 Accuracy 0.7926\n",
            "Epoch 21 Loss 0.7821 Accuracy 0.7902\n",
            "Time taken for 1 epoch: 25.92 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.7001 Accuracy 0.8113\n",
            "Epoch 22 Batch 50 Loss 0.7103 Accuracy 0.8064\n",
            "Epoch 22 Batch 100 Loss 0.7155 Accuracy 0.8055\n",
            "Epoch 22 Batch 150 Loss 0.7322 Accuracy 0.8014\n",
            "Epoch 22 Batch 200 Loss 0.7474 Accuracy 0.7978\n",
            "Epoch 22 Loss 0.7590 Accuracy 0.7949\n",
            "Time taken for 1 epoch: 25.95 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.6028 Accuracy 0.8382\n",
            "Epoch 23 Batch 50 Loss 0.6840 Accuracy 0.8133\n",
            "Epoch 23 Batch 100 Loss 0.6941 Accuracy 0.8113\n",
            "Epoch 23 Batch 150 Loss 0.7057 Accuracy 0.8082\n",
            "Epoch 23 Batch 200 Loss 0.7224 Accuracy 0.8039\n",
            "Epoch 23 Loss 0.7329 Accuracy 0.8018\n",
            "Time taken for 1 epoch: 26.02 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.7387 Accuracy 0.7994\n",
            "Epoch 24 Batch 50 Loss 0.6735 Accuracy 0.8152\n",
            "Epoch 24 Batch 100 Loss 0.6789 Accuracy 0.8138\n",
            "Epoch 24 Batch 150 Loss 0.6877 Accuracy 0.8114\n",
            "Epoch 24 Batch 200 Loss 0.7018 Accuracy 0.8079\n",
            "Epoch 24 Loss 0.7129 Accuracy 0.8050\n",
            "Time taken for 1 epoch: 25.72 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.6688 Accuracy 0.8118\n",
            "Epoch 25 Batch 50 Loss 0.6531 Accuracy 0.8186\n",
            "Epoch 25 Batch 100 Loss 0.6580 Accuracy 0.8181\n",
            "Epoch 25 Batch 150 Loss 0.6688 Accuracy 0.8155\n",
            "Epoch 25 Batch 200 Loss 0.6823 Accuracy 0.8123\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-17\n",
            "Epoch 25 Loss 0.6907 Accuracy 0.8103\n",
            "Time taken for 1 epoch: 26.17 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.6233 Accuracy 0.8160\n",
            "Epoch 26 Batch 50 Loss 0.6245 Accuracy 0.8253\n",
            "Epoch 26 Batch 100 Loss 0.6298 Accuracy 0.8245\n",
            "Epoch 26 Batch 150 Loss 0.6451 Accuracy 0.8210\n",
            "Epoch 26 Batch 200 Loss 0.6630 Accuracy 0.8169\n",
            "Epoch 26 Loss 0.6690 Accuracy 0.8158\n",
            "Time taken for 1 epoch: 26.05 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.5772 Accuracy 0.8346\n",
            "Epoch 27 Batch 50 Loss 0.5956 Accuracy 0.8345\n",
            "Epoch 27 Batch 100 Loss 0.6131 Accuracy 0.8292\n",
            "Epoch 27 Batch 150 Loss 0.6294 Accuracy 0.8249\n",
            "Epoch 27 Batch 200 Loss 0.6413 Accuracy 0.8217\n",
            "Epoch 27 Loss 0.6532 Accuracy 0.8190\n",
            "Time taken for 1 epoch: 25.78 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.5512 Accuracy 0.8505\n",
            "Epoch 28 Batch 50 Loss 0.6031 Accuracy 0.8317\n",
            "Epoch 28 Batch 100 Loss 0.5995 Accuracy 0.8325\n",
            "Epoch 28 Batch 150 Loss 0.6087 Accuracy 0.8304\n",
            "Epoch 28 Batch 200 Loss 0.6205 Accuracy 0.8273\n",
            "Epoch 28 Loss 0.6318 Accuracy 0.8245\n",
            "Time taken for 1 epoch: 26.36 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.5135 Accuracy 0.8604\n",
            "Epoch 29 Batch 50 Loss 0.5663 Accuracy 0.8407\n",
            "Epoch 29 Batch 100 Loss 0.5780 Accuracy 0.8372\n",
            "Epoch 29 Batch 150 Loss 0.5892 Accuracy 0.8344\n",
            "Epoch 29 Batch 200 Loss 0.6043 Accuracy 0.8308\n",
            "Epoch 29 Loss 0.6126 Accuracy 0.8288\n",
            "Time taken for 1 epoch: 26.29 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.5536 Accuracy 0.8458\n",
            "Epoch 30 Batch 50 Loss 0.5586 Accuracy 0.8431\n",
            "Epoch 30 Batch 100 Loss 0.5645 Accuracy 0.8407\n",
            "Epoch 30 Batch 150 Loss 0.5757 Accuracy 0.8379\n",
            "Epoch 30 Batch 200 Loss 0.5872 Accuracy 0.8350\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-18\n",
            "Epoch 30 Loss 0.5979 Accuracy 0.8323\n",
            "Time taken for 1 epoch: 26.21 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "y1dtXgy3C-YH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "rTYqMDLnC-aR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "AF5J3Yt3C-c7"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions, _ = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      predictions = predictions[:, -1:, :]\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    text = tokenizers.en.detokenize(output)[0]\n",
        "\n",
        "    tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "    _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "\n",
        "    return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "388EohgWC-fh"
      },
      "outputs": [],
      "source": [
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "yvc5f42BC-iY"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "mGxA5g8OC-nt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "b3xtoOZtC-0a"
      },
      "outputs": [],
      "source": [
        "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
        "  translated_tokens = translated_tokens[1:]\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.matshow(attention)\n",
        "  ax.set_xticks(range(len(in_tokens)))\n",
        "  ax.set_yticks(range(len(translated_tokens)))\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "  ax.set_xticklabels(\n",
        "      labels, rotation=90)\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
        "  ax.set_yticklabels(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "TXh-W_ezC-3B"
      },
      "outputs": [],
      "source": [
        "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
        "  in_tokens = tf.convert_to_tensor([sentence])\n",
        "  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
        "  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
        "  in_tokens\n",
        "\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "  for h, head in enumerate(attention_heads):\n",
        "    ax = fig.add_subplot(2, 4, h+1)\n",
        "\n",
        "    plot_attention_head(in_tokens, translated_tokens, head)\n",
        "\n",
        "    ax.set_xlabel(f'Head {h+1}')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cVrriDq0UYR1"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'este é um problema que temos que resolver.'\n",
        "ground_truth = 'this is a problem we have to solve .'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights['decoder_layer4_block2'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "Y4CBagnXTKGS",
        "outputId": "6991021e-222f-4478-def2-aaeb6a38d0ca"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : este é um problema que temos que resolver.\n",
            "Prediction     : this is a problem that we have to fix .\n",
            "Ground truth   : this is a problem we have to solve .\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uDamPN7VUcGw"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pt_sentences, en_sentences in x_test.shuffle(BUFFER_SIZE).batch(20).take(1):\n",
        "  for pt, en in zip(pt_sentences.numpy(), en_sentences.numpy()):\n",
        "    sentence = pt.decode('utf-8')\n",
        "    ground_truth = en.decode('utf-8')\n",
        "\n",
        "    translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "    print_translation(sentence, translated_text, ground_truth)\n",
        "    print('----------------------------------------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx8Cvm9RTExN",
        "outputId": "4ed6f320-a8cc-43bf-cc40-df0938830cdb"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : em 2009 , em itália , l'aquila , também tiveram um grande terramoto .\n",
            "Prediction     : in 2009 , lo in italy , i also had a great earthquake .\n",
            "Ground truth   : in 2009 , in italy , l'aquila , also they had a big earthquake .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : `` temos de ter cuidado com a palavra `` '' hacker '' '' . ''\n",
            "Prediction     : ` ` we have to be careful with the word ` ` ' ' there ' suel . ' '\n",
            "Ground truth   : so you have to be careful with hacker .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : gosto de pensar nisto um pouco como uma boneca russa .\n",
            "Prediction     : i like to think of this as a little boosa .\n",
            "Ground truth   : i like to think of it a bit like a russian doll .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : ( aplausos )\n",
            "Prediction     : ( applause )\n",
            "Ground truth   : ( applause )\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : o chris alegou de forma brilhante .\n",
            "Prediction     : chris andersond way .\n",
            "Ground truth   : chris argued brilliantly .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : `` `` '' está preso num espasmo e é extremamente doloroso . ''\n",
            "Prediction     : ` ` ` ` ' ' this was almost completely broken and van . ' '\n",
            "Ground truth   : it 's fixed in a clenched spasm and it 's excruciatingly painful .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : muito obrigada .\n",
            "Prediction     : thank you very much .\n",
            "Ground truth   : thank you so much .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : penso agora que nunca fui pura .\n",
            "Prediction     : now i think i ' ve never turned out .\n",
            "Ground truth   : i think now i never was pure .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : como é possível que as crianças façam análises estatísticas ?\n",
            "Prediction     : how can kids look at statistics ?\n",
            "Ground truth   : how could it be that children are doing statistics ?\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : então , como é que isto aconteceu ?\n",
            "Prediction     : so how did this happen ?\n",
            "Ground truth   : so , how did this happen ?\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : as probabilidades de vos sairem estas cartas são de 4165 para 1 .\n",
            "Prediction     : your chances of you are coming from these letters are from 3165 .\n",
            "Ground truth   : the odds of getting it are 4,165 to one .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : fizeram a matemática simples , a média , etc .\n",
            "Prediction     : they did the simple math , on average , etc .\n",
            "Ground truth   : they do the simple mathematics , averaging , etc. , etc .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : na clínica em que sou consultor , conheço provavelmente uma centena ou mais do que uma centena de médicos e enfermeiros e outras equipas hospitalares ou de cuidados de saúde todos os anos .\n",
            "Prediction     : on the clinic that i ' m probably to talk to a cruipot or more than a c anata and a clevelation and other areas of getting through time for every year .\n",
            "Ground truth   : in my practice as a consultant , i meet probably a hundred or more than a hundred doctors and nurses and other hospital or healthcare staff every year .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : então contactei a noaa , e perguntei se podiam partilhar os dados sobre o tsunami , e traduzi-os nisto .\n",
            "Prediction     : so i noticed that last caaa , and i asked the data about share their tsunami , and i curator them .\n",
            "Ground truth   : so i contacted noaa , and i asked if they 'd share their data on the tsunami , and translated it into this .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : o que acontece quando se impede às pessoas o acesso à troca , à capacidade de trocar e se especializar ?\n",
            "Prediction     : what happens when people exchange access to the exchange , the capacity and the ability to reproduction ?\n",
            "Ground truth   : what happens when you cut people off from exchange , from the ability to exchange and specialize ?\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : a liderança é uma escolha . não é uma classificação .\n",
            "Prediction     : the leadership is not a peer grading .\n",
            "Ground truth   : leadership is a choice . it is not a rank .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : têm apenas que ler o que veem . ok ?\n",
            "Prediction     : they have only i have to read what . okay ?\n",
            "Ground truth   : all you have to do is read what you see . right ?\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : toda a gente no mundo respeita nelson mandela , todos veneram nelson mandela .\n",
            "Prediction     : everybody ' s respected the world nelson mandela , everyone has found bed condition .\n",
            "Ground truth   : everyone in the world respects nelson mandela , everyone reveres nelson mandela .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : parece que o segredo era brócolos .\n",
            "Prediction     : it looks like the secret that was my motorcy .\n",
            "Ground truth   : well it turns out that the secret was broccoli .\n",
            "----------------------------------------------------------------------------------------------------------\n",
            "Input:         : e este é apenas um de centenas e centenas de estudos feitos nos últimos 20 anos que realmente demonstrou isso .\n",
            "Prediction     : and this is just a hundreds of hundred and hundreds of studies made of the last 20 years that actually has beentronomy .\n",
            "Ground truth   : and this is just one of hundreds and hundreds of studies over the last 20 years that 's actually demonstrated it .\n",
            "----------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N7wT4ZNLTKES"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SbiJQnz_TKB8"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finito"
      ],
      "metadata": {
        "id": "l45FTiwfVD9W"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW08.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzSpUqJgEwoYTilg0dUohc",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}